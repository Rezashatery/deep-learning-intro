{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a famous library named TensorFlow (which is now available in version 2+) and in particular one of its subpackages, Keras, together with some utilities libraries like: \n",
    "\n",
    "- Matplotlib (for visualization);\n",
    "- Numpy (to work with arrays);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Runtime\n",
    "\n",
    "Neural Network training requires high parallel computation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 10:56:46.899616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Check tensorflow version (must be >2!)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a dataset\n",
    "\n",
    "To train a Neural Network model, we will need to load in memory a dataset. You can load it in lots of ways, depending on the time of data that you need.\n",
    "\n",
    "we will use built-in data on keras. In particoular, we are interested in:\n",
    "\n",
    "- Fashion-MNIST: It is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n",
    "\n",
    "we will download the dataset locally and experiment with it.\n",
    "You can visualize the data [here](https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=fashion_mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimension: Input (60000, 28, 28), Output (60000,)\n",
      "Test set dimension: Input (10000, 28, 28), Output (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Import keras dataset Fashion Mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Check data dimensionality\n",
    "print(f\"Training set dimension: Input {x_train.shape}, Output {y_train.shape}\")\n",
    "print(f\"Test set dimension: Input {x_test.shape}, Output {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like our input data to lies in the interval $[0, 1]$. If our data does not lies in this interval, we can transform it as:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - x_{min}}{x_{max}-x_{min}}\n",
    "$$\n",
    "\n",
    "Where $x_{min} = \\min(x)$, $x_{max} = \\max(x)$. Note that $x'$ always lies in the interval $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization function\n",
    "normalize_data = (lambda X: ((X - X.min()) / (X.max() - X.min())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is called normalization(min-max feature scaling) and allow us to work easily with the data, speeding up the computation and in some case improving even the results of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (train) data lies in the interval [0, 255]\n",
      "Input (test) data lies in the interval [0, 255]\n",
      "\n",
      "\n",
      "Input (train) data lies in the interval [0.0, 1.0]\n",
      "Input (test) data lies in the interval [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input (train) data lies in the interval [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Input (test) data lies in the interval [{x_test.min()}, {x_test.max()}]\")\n",
    "    \n",
    "x_train = normalize_data(x_train)\n",
    "x_test = normalize_data(x_test)\n",
    "\n",
    "# Check the interval after normalization\n",
    "print(\"\\n\")\n",
    "print(f\"Input (train) data lies in the interval [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Input (test) data lies in the interval [{x_test.min()}, {x_test.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y[0]: 9\n",
      "y[0] after the one-hot encoding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y[0]: {y_train[0]}\")\n",
    "\n",
    "# One hot encode the output variables (needed to compute the output)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "n_classes = len(y_train[0])\n",
    "\n",
    "print(f\"y[0] after the one-hot encoding: {y_train[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
